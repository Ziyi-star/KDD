{\rtf1\ansi\ansicpg1252\cocoartf2639
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import math\
from collections import Counter, defaultdict\
import numpy as np\
import random # Just needed for 6ECTS\
\
\
def euclidean_dist(x, y):\
    xnp = np.array(x)\
    ynp = np.array(y)\
    #print ('xnp, ynp', xnp, ynp)\
    return np.linalg.norm(ynp-xnp)\
\
def split(X, Y, n=5):\
    "Split the training points and labels into 5 equal sized parts. Just needed for 6 ECTS."\
    m = list(range(len(X)))\
    X = np.array(X)\
    Y = np.array(Y)\
    indices = [np.array(m[i::n]) for i in range(n)]\
    X = [X[i].tolist() for i in indices]\
    Y = [Y[i].tolist() for i in indices]\
    return X, Y\
\
class KNN():\
    def __init__(self, dist_fun=euclidean_dist):\
        self.dist_fun = dist_fun\
        self.strategies = ['majority', 'inverse_squared_distance', 'inverse_avg_distance', 'distribution']\
    \
    def train(self, X, Y, ks=None):\
        """ Train this classifier. Takes a list of samples X and a list of class-labels Y.\
        Each sample is a list of numeric values. Each label is a string.\
        The parameter ks ist just needed for 6ECTS."""\
        assert(len(X) == len(Y))\
        \
        self.data_min = np.min (X, axis = 0)\
        #print( "self.data_min",self.data_min, type(self.data_min)) # [4.3 2.2 1.  0.1] <class 'numpy.ndarray'>\
        self.data_max = np.max (X, axis = 0)\
        #print( "self.data_max",self.data_max, type(self.data_max)) # [7.9 4.4 6.9 2.5] <class 'numpy.ndarray'>\
        \
        X_train_normalized = (X - self.data_min) / (self.data_max - self.data_min)\
        #print( "X_train_normalized",X_train_normalized, type(X_train_normalized))# [[0.11111111 0.45454545 0.05084746 0.04166667]...\
        # <class 'numpy.ndarray'>\
        \
        self.X_train = X_train_normalized\
        self.Y_train = Y\
        \
        \
                        \
# TODO habe aus X -> X_test gemacht (auch im return)  \
    def predict(self, X_test, k=3, strategy='majority', best_combination=False):\
        # hier werden die X_test Datenpunkte eingegeben und das k festgelegt\
        """ Takes a list of samples X. Returns a list of predicted labels for the samples.\
        The parameter best_combination ist just needed for 6ECTS."""\
        # dann wird eine Liste (?) mit den vorhergesagten Labels zur\'fcckgegeben\
        return [self.predict_sample(x, k, strategy) for x in X_test]\
    \
    def predict_sample(self, x, k=3, strategy='majority'):\
        """ Predicts the label for a single sample x. """\
        print("Neuer Punkt x", x, type(x)) # 6.0, 2.2, 5.0, 1.5] <class 'list'>\
        \
        # Punkt normalisieren\
        x_normalized = (x - self.data_min) / (self.data_max - self.data_min)\
        ##print( "x_normalized",x_normalized, type(x_normalized)) # 1.) [0.47222222 0.  0.6779661  0.58333333] <class 'numpy.ndarray'> 2.)...\
        \
        number_of_rows = len(X_train)\
        #print("number_of_rows", number_of_rows) #97\
        \
        # Distanz zu den Neighbors berechnen \
        neighbors = []\
        for i in range(number_of_rows):\
            #print("X_train[i]",X_train[i]) # 1.) [4.7, 3.2, 1.3, 0.2], ...\
            #print("x",x) # 1.) bei i = 0- 96:  [6.0, 2.2, 5.0, 1.5], 2.) bei i = 0- 96: ...\
            distance = self.dist_fun(x_normalized, self.X_train[i])\
            #print("distance", distance) # 26.656894042629947, ...\
            neighbors.append((distance, Y_train[i]))\
        #print("neighbors", neighbors) # [(1.0117773353515311, 'setosa'), (0.98...\
        \
        # sort\
        neighbors = sorted(neighbors)    \
\
        # Die nachsten k Nachbarn auswaehlen und in numpy array konvertieren\
        k_neighbors = neighbors[:k]   \
        k_neighbors = np.array(k_neighbors)\
        #print("k_neighbors", k_neighbors) # 1.)['0.10133244322872142' 'versicolor'] ['0.2142601761355073' 'virginica']...\
\
        # Listen mit den unique labels erstellen und den counts (counts fuer majority)\
        labels = k_neighbors[:, 1]\
        #print("labels", labels) # 1.) ['versicolor' 'virginica' 'virginica'], 2.) ...     \
        uniq_label, counts = np.unique(labels, return_counts=True)\
        #print("uniq_label", uniq_label) # 1.) ['versicolor' 'virginica'], 2.)...\
        #print("counts", counts, type(counts)) # 1.) counts [1 2] <class 'numpy.ndarray'>     \
        \
        \
        if strategy=='majority':  \
            # Eine einfache Mehrheitsabstimmung unter den Nachbarn\
        \
            ##print("uniq_label", uniq_label) # 1.) ['versicolor' 'virginica'], 2.)...\
            ##print("counts", counts, type(counts)) # 1.) counts [1 2] <class 'numpy.ndarray'>     \
        \
            pred = uniq_label[counts.argmax()]\
            ##print("pred", pred) # 1.) virginica, 2.) ...\
 \
\
        elif strategy == 'inverse_squared_distance':\
            # Jeder Nachbar wird mit dem inversen Quadrat der Distanz gewichtet.\
\
            weighted_unique_k_neighbors = []          \
            for i in uniq_label:\
                #print("i", i)\
                weight_i = 0\
                for j in k_neighbors:\
                    #print("j[1]", j[1])\
                    if j[1]== i:\
                        ##print("dist",j[1], float(j[0]))\
                        weight_j = 1/((float(j[0])**2))\
                        ##print("weight_j", i, weight_j)\
                        weight_i += weight_j\
                        #print("weight_i", weight_i) # 1.) 97.38744495294013, 2.) ...\
                #print("weight_i", weight_i)\
                weighted_unique_k_neighbors.append((i, weight_i))\
            ##print("weighted_unique_k_neighbors", weighted_unique_k_neighbors) # [('versicolor', 97.38744495294013), ('virginica', 36.28786031018865)]\
           \
            maxi = 0\
            for i in weighted_unique_k_neighbors:\
                if i[1] > maxi:\
                    maxi = i[1]\
                    pred = i[0]\
\
            ##print("pred", pred) # 1.) virginica, 2.) ...\
        \
        \
        elif strategy == 'inverse_avg_distance':\
            # Die Stimmen einer Klasse werden mit dem Inversen ihrer Durchschnittsdistanz gewichtet.\
            \
            # Inverse der Durchschnittsdistanz berechnen\
            dict_weighted_labels = \{\}\
            for i in uniq_label: # jedes label\
                #print("i", i) # 1.) versicolor, 2.)virginica, 3.) ...\
                dist_sum = 0\
                label_count = 0\
                for j in neighbors: #jeder datenpunkt des gesamten datensatzes\
                    #print("j[1]", j[1]) #1.) versicolor, 2.) virginica, 3.) virginica 4.)...\
                    if j[1]== i:\
                        dist_sum += (float(j[0]))\
                        label_count +=1\
                        #print("dist_avgj", dist_avg) # 1.) \
                #print("i, dist_sum", i, dist_sum)\
                #print("label_count", label_count)\
                dist_avg = dist_sum/label_count\
                #print("i, dist_avg", i, dist_avg) # 1.) versicolor 0.36592428973184443, virginica 0.5367953627062749,  2.) ...\
                # die Inverse ihrer Durchschnittsdistanz:\
                weight_dist_avg = 1/(dist_avg**2)\
                #print("i, weight_dist_avg", i, weight_dist_avg)\
                dict_weighted_labels[i]=weight_dist_avg\
            ##print("dict_weighted_labels", dict_weighted_labels)\
            \
            # Weights der Stimmen der Klassen aufsummieren\
            unique_k_neighbors_with_label_weight = \{\}\
            for i in uniq_label: # jedes label\
                #print("i", i) # 1.) versicolor, 2.)virginica, 3.) ...\
                label_weight_i = 0\
                for j in k_neighbors:\
                    #print("j[1]", j[1]) #1.) versicolor, 2.) virginica, 3.) virginica \
                    if j[1]== i:\
                        #print("dict_weighted_labels[i]", dict_weighted_labels[i]) \
                        label_weight_i+= dict_weighted_labels[i]\
                        #print ("label_weight_i in if", i, label_weight_i)\
                    #print ("label_weight_i", i, label_weight_i)\
                    unique_k_neighbors_with_label_weight[i]=label_weight_i\
            ##print("unique_k_neighbors_with_label_weight", unique_k_neighbors_with_label_weight)\
            \
            pred = max(unique_k_neighbors_with_label_weight, key = unique_k_neighbors_with_label_weight.get)\
            ##print("pred",pred)\
            \
        \
        elif strategy == 'distribution':\
            #Eine Mehrheitsabstimmung gewichtet nach der Verteilung der Klassen\
            \
            print("uniq_label", uniq_label)\
            # Mehrheitsverhaeltnisse berechnen\
            dict_distribution = \{\}\
            for i in uniq_label: # jedes label\
                #print("i", i) # 1.) versicolor, 2.)virginica, 3.) ...\
                label_count = 0\
                for j in neighbors: #jeder datenpunkt des gesamten datensatzes\
                    #print("j[1]", j[1]) #1.) versicolor, 2.) virginica, 3.) virginica 4.)...\
                    if j[1]== i:\
                        label_count +=1\
                        #print("label_countin if ", i, label_count) # 1.)\
                #print("label_count nach for", i, label_count)\
                dict_distribution[i]=label_count/number_of_rows\
            print("dict_distribution", dict_distribution)\
            \
            # ...\
            \
            \
            \
            \
            \
            \
            pred = None\
        \
        \
        return pred\
       \
        \
        \
clf = KNN()\
clf.train(X_train, Y_train)\
for strategy in clf.strategies:\
    predictions = clf.predict(X_test, strategy=strategy, k=3)\
# naechste Zeile ist von mir:\
    #print("predictions",predictions , type(predictions)) # ['virginica', 'versicolor', ... <class 'list'>\
    print('Accuracy of strategy \{\}: \{\}'.format(strategy, accuracy(predictions, Y_test)))\
    labels, matrix = confusion_matrix(predictions, Y_test)\
    print('Confusion matrix:')\
    print('\\n'.join([str(row) for row in matrix]))\
    print('----------')}