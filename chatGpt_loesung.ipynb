{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae83893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2\n",
    "\n",
    "def entropy(target_col):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a dataset.\n",
    "    The only parameter of this function is the target_col parameter which specifies the target column\n",
    "    \"\"\"\n",
    "    elements, counts = np.unique(target_col, return_counts=True)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy\n",
    "\n",
    "def InfoGain(data, split_attribute_name, target_name=\"class\"):\n",
    "    \"\"\"\n",
    "    Calculate the information gain of a dataset. This function takes three parameters:\n",
    "    1. data = The dataset for whose feature the IG should be calculated\n",
    "    2. split_attribute_name = the name of the feature for which the information gain should be calculated\n",
    "    3. target_name = the name of the target feature. The default value is \"class\"\n",
    "    \"\"\"\n",
    "    # Calculate the entropy of the total dataset\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    \n",
    "    # Calculate the values and the corresponding counts for the split attribute \n",
    "    vals, counts= np.unique(data[split_attribute_name],return_counts=True)\n",
    "    \n",
    "    # Calculate the weighted entropy\n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "    \n",
    "    # Calculate the information gain\n",
    "    Information_Gain = total_entropy - Weighted_Entropy\n",
    "    return Information_Gain\n",
    "\n",
    "# Example usage (Note: this will not run here as we don't have the actual dataset loaded)\n",
    "# Assuming df is our DataFrame and 'class' is the target column\n",
    "# info_gain_example = InfoGain(df, 'some_feature', 'class')\n",
    "# print(info_gain_example)\n",
    "\n",
    "# This code defines the entropy and information gain calculation. \n",
    "# The next step will be to use these in the actual ID3 algorithm implementation to build the decision tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c758b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(data, originaldata, features, target_attribute_name=\"class\", parent_node_class = None):\n",
    "    \"\"\"\n",
    "    ID3 Algorithm: Builds the decision tree based on maximizing information gain.\n",
    "    \n",
    "    - data: the current subset of the dataset being processed (Pandas DataFrame).\n",
    "    - originaldata: the original dataset before any splits (Pandas DataFrame).\n",
    "    - features: the list of feature names (columns) that are candidates for splitting.\n",
    "    - target_attribute_name: the name of the target attribute (string).\n",
    "    - parent_node_class: the majority class of the parent node for the current node being processed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If all target_values have the same value, return this value\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    \n",
    "    # If the dataset is empty, return the mode target feature value in the original dataset\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name], return_counts=True)[1])]\n",
    "    \n",
    "    # If the feature space is empty, return the mode target feature value of the direct parent node \n",
    "    elif len(features) == 0:\n",
    "        return parent_node_class\n",
    "    \n",
    "    else:\n",
    "        # Set the default value for this node --> The mode target feature value of the current node\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]\n",
    "        \n",
    "        # Select the feature which best splits the dataset\n",
    "        item_values = [InfoGain(data, feature, target_attribute_name) for feature in features] # Return the information gain values for the features in the dataset\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        \n",
    "        # Create the tree structure. The root gets the name of the best feature\n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        # Remove the feature with the best info gain from the feature space\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        # Grow a branch under the root node for each possible value of the root node feature\n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            # Call the ID3 algorithm for each of those sub_datasets with the new parameters\n",
    "            subtree = ID3(sub_data, originaldata, features, target_attribute_name, parent_node_class)\n",
    "            \n",
    "            # Add the subtree, under the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return tree\n",
    "\n",
    "# Note: This code is a template and won't run here as it requires a dataset (df) and a list of features.\n",
    "# To use it, you would need to call it like this:\n",
    "# tree = ID3(df, df, list(df.columns[:-1]), 'class')\n",
    "# This assumes your DataFrame is named df and the last column is the target variable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
